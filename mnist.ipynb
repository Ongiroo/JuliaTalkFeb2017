{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Note: you may have to add/clone/checkout some of these packages\n",
    "Notebook based on Tom Berloff works: http://www.breloff.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plots.GRBackend()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this re-exports Transformations, StochasticOptimization, Penalties, and ObjectiveFunctions\n",
    "using Learn\n",
    "using MLPlots\n",
    "\n",
    "# my version of ML iteration.  Hopefully will be replaced with what's currently in MLDataUtils dev branch\n",
    "using StochasticOptimization.Iteration\n",
    "\n",
    "import MLDataUtils: rescale!\n",
    "\n",
    "# for loading the data\n",
    "import MNIST\n",
    "\n",
    "# for plotting\n",
    "using StatPlots, MLPlots\n",
    "#gr(leg=false, linealpha=0.5, legendfont=font(7), fmt=:png)\n",
    "gr(leg=false, linealpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition to_one_hot(AbstractArray{T<:Any, 1}) in module Main at In[2]:4 overwritten at In[17]:4.\n",
      "WARNING: Method definition my_test_loss(Any, Any) in module Main at In[2]:15 overwritten at In[17]:15.\n",
      "WARNING: Method definition my_test_loss(Any, Any, Any) in module Main at In[2]:15 overwritten at In[17]:15.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "my_test_loss (generic function with 2 methods)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a one-hot matrix given class labels\n",
    "# TODO: this should be added as a utility in MLDataUtils\n",
    "function to_one_hot(y::AbstractVector)\n",
    "    yint = map(yi->round(Int,yi)+1, y)\n",
    "    nclasses = maximum(yint)\n",
    "    hot = zeros(Float64, nclasses, length(y))\n",
    "    for (i,yi) in enumerate(yint)\n",
    "        hot[yi,i] = 1.0\n",
    "    end\n",
    "    hot\n",
    "end\n",
    "\n",
    "# randomly pick a subset of testdata (size = totcount) and compute the total loss\n",
    "function my_test_loss(obj, testdata, totcount = 500)\n",
    "    totloss = 0.0\n",
    "    totcorrect = 0\n",
    "    for (x,y) in each_obs(rand(each_obs(testdata), totcount))\n",
    "        totloss += transform!(obj,y,x)\n",
    "\n",
    "        # logistic version:\n",
    "        # ŷ = output_value(obj.transformation)[1]\n",
    "        # correct = (ŷ > 0.5 && y > 0.5) || (ŷ <= 0.5 && y < 0.5)\n",
    "\n",
    "        # softmax version:\n",
    "        ŷ = output_value(obj.transformation)\n",
    "        chosen_idx = indmax(ŷ)\n",
    "        correct = y[chosen_idx] > 0\n",
    "\n",
    "        totcorrect += correct\n",
    "    end\n",
    "    totloss, totcorrect/totcount\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# our data:\n",
    "x_train, y_train = MNIST.traindata()\n",
    "x_test, y_test = MNIST.testdata()\n",
    "\n",
    "# normalize the input data given μ/σ for the input training data\n",
    "# note: scale both train and test sets using the train data\n",
    "μ, σ = rescale!(x_train)\n",
    "rescale!(x_test, μ, σ)\n",
    "# xmin, xmax = extrema(x_train)\n",
    "# x_train .= 2 .* (x_train .- xmin) ./ (xmax - xmin) .- 1\n",
    "# x_test .= 2 .* (x_test .- xmin) ./ (xmax - xmin) .- 1\n",
    "\n",
    "# convert y data to one-hot\n",
    "y_train, y_test = map(to_one_hot, (y_train, y_test))\n",
    "\n",
    "# optional: limit to only 0/1 digits for easier training\n",
    "# to_isone(y::AbstractVector) = (z = Array(eltype(y), 1, length(y)); map!(yi->float(yi==1.0), z, y))\n",
    "# y_train, y_test = map(to_isone, (y_train, y_test))\n",
    "# train = filterobs(i -> y_train[i] < 1.5, x_train, y_train)\n",
    "# test = filterobs(i -> y_test[i] < 1.5, x_test, y_test)\n",
    "\n",
    "# store as tuples to make it easier\n",
    "train = (x_train, y_train)\n",
    "test = (x_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct our model and objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectiveFunctions.RegularizedObjective{Transformations.Chain{Float64,Transformations.Params{SubArray{Float64,1,Array{Float64,1},Tuple{UnitRange{Int64}},true},Tuple{},Tuple{}},Transformations.NoPreprocessing},ObjectiveFunctions.CrossEntropy{Float64},PenaltyFunctions.NoPenalty}(Chain{Float64}(\n",
       "   Affine{784-->50}\n",
       "   softplus{50}\n",
       "   Affine{50-->50}\n",
       "   softplus{50}\n",
       "   Affine{50-->10}\n",
       "   softmax{10}\n",
       ") ,ObjectiveFunctions.CrossEntropy{Float64}(10,Transformations.InputNode{:+,Float64,1}(Transformations.Node[Transformations.OutputNode{Float64,1}(Transformations.Node[Transformations.InputNode{:+,Float64,1}(#= circular reference @-4 =#)],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],Dict{Transformations.OutputNode{Float64,1},Int64}()),Transformations.InputNode{:+,Float64,1}(Transformations.Node[],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],Dict{Transformations.OutputNode{Float64,1},Int64}()),Transformations.OutputNode{Float64,1}(Transformations.Node[],[0.0],[1.0])),NoPenalty)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activation Functions => :logistic, :tanh, :softsign, :relu, :softplus, :sinusoid, :gaussian, :threshold, :sign\n",
    "\n",
    "nin, nh, nout = 784, [50,50], 10\n",
    "\n",
    "# this is our gradient calculation method\n",
    "\n",
    "#grad_calc = :backprop\n",
    "grad_calc = :dfa\n",
    "\n",
    "# create a feedforward neural net with softplus activations and softmax output\n",
    "t = nnet(nin, nout, nh, :softplus, :softmax, grad_calc=grad_calc)\n",
    "\n",
    "# create an objective function with L2 penalty and an implicit cross entropy loss layer\n",
    "penalty = NoPenalty()\n",
    "obj = objective(t, penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optional: set up plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "(trainloss,trainaccuracy) = (493.2691957699173,0.1)\n",
      "(testloss,testaccuracy) = (512.8417912776653,0.085)\n"
     ]
    }
   ],
   "source": [
    "# the parts of the plot\n",
    "chainplt = ChainPlot(t, maxn=10, tickfont=font(5))\n",
    "lossplt = TracePlot(2, title=\"Loss\", ylim=(0,Inf), leg=true, lab=[\"Train\" \"Test\"])\n",
    "accuracyplt = TracePlot(2, title=\"Accuracy\", ylim=(0.4,1), leg=true, lab=[\"Train\" \"Test\"])\n",
    "hmplt = heatmap(rand(28,28), ratio=1, title=\"outgoing wgt\")\n",
    "hmplt2 = heatmap(rand(28,28), ratio=1, title=\"input grad\")\n",
    "\n",
    "# put together the full plot... a ChainPlot with loss, accuracy, and the heatmap\n",
    "plot(\n",
    "    chainplt.plt,\n",
    "    lossplt.plt,\n",
    "    accuracyplt.plt,\n",
    "    hmplt,\n",
    "    hmplt2,\n",
    "    size = (1200,800),\n",
    "    layout=@layout([a{0.8h}; grid(1,2){0.75w} grid(1,2)])\n",
    ")\n",
    "\n",
    "anim = nothing\n",
    "#anim = Animation()\n",
    "\n",
    "# this is our custom callback which will be called on every 100 iterations\n",
    "# note: we do the plotting here.\n",
    "tracer = IterFunction((obj, i) -> begin\n",
    "    # sample points from the test set and compute/save the loss\n",
    "    @show i\n",
    "    if mod1(i,500)==500\n",
    "        # training loss\n",
    "        trainloss, trainaccuracy = my_test_loss(obj, train, 200)\n",
    "        @show trainloss, trainaccuracy\n",
    "        \n",
    "        testloss, testaccuracy = my_test_loss(obj, test, 200)\n",
    "        @show testloss, testaccuracy\n",
    "        \n",
    "        push!(lossplt, i, [trainloss, testloss])\n",
    "        push!(accuracyplt, i, [trainaccuracy, testaccuracy])\n",
    "    end\n",
    "\n",
    "    # add transformation data\n",
    "    update!(chainplt)\n",
    "\n",
    "    # update the heatmap of the total outgoing weight from each pixel\n",
    "    t1 = isa(t[1], InputNorm) ? t[2] : t[1]\n",
    "    pixel_importance = reshape(sum(t1.params.views[1],1), 28, 28)\n",
    "    hmplt[1][1][:z].surf[:] = pixel_importance\n",
    "    \n",
    "    pixel_importance = reshape(abs(input_grad(t)),28,28)  # another possible metric\n",
    "    hmplt2[1][1][:z].surf[:] = pixel_importance\n",
    "\n",
    "    # handle animation frames/output\n",
    "    anim == nothing || frame(anim)\n",
    "\n",
    "    # update the plot display\n",
    "    gui()\n",
    "    #inline()\n",
    "end, every=100)\n",
    "\n",
    "# trace once before we start learning to see initial values\n",
    "tracer.f(obj, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a MetaLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StochasticOptimization.MetaLearner{Tuple{StochasticOptimization.GradientLearner{StochasticOptimization.FixedLR,StochasticOptimization.SGD{Float64},StochasticOptimization.GradientAverager},StochasticOptimization.IterFunction,StochasticOptimization.MaxIter}}((StochasticOptimization.GradientLearner{StochasticOptimization.FixedLR,StochasticOptimization.SGD{Float64},StochasticOptimization.GradientAverager}(StochasticOptimization.FixedLR(0.001),StochasticOptimization.SGD{Float64}(0.7,#undef),StochasticOptimization.GradientAverager(#undef)),StochasticOptimization.IterFunction(#9,100),StochasticOptimization.MaxIter(1500)))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = make_learner(\n",
    "    # averages the gradient over minibatches, updating params using the Adam method\n",
    "    GradientLearner(1e-3, SGD(0.7)),\n",
    "\n",
    "    # our custom iteration method\n",
    "    tracer,\n",
    "\n",
    "    # shorthand to add a MaxIter(10000)\n",
    "    maxiter = 1500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 100\n",
      "i = 200\n",
      "i = 300\n",
      "i = 400\n",
      "i = 500\n",
      "(trainloss,trainaccuracy) = (129.6869385214654,0.835)\n",
      "(testloss,testaccuracy) = (145.61546018051897,0.83)\n",
      "i = 600\n",
      "i = 700\n",
      "i = 800\n",
      "i = 900\n",
      "i = 1000\n",
      "(trainloss,trainaccuracy) = (230.498822059817,0.825)\n",
      "(testloss,testaccuracy) = (265.36885330861423,0.82)\n",
      "i = 1100\n",
      "i = 1200\n",
      "i = 1300\n",
      "i = 1400\n",
      "i = 1500\n",
      "(trainloss,trainaccuracy) = (209.0158975101924,0.84)\n",
      "(testloss,testaccuracy) = (261.7942428099709,0.845)\n"
     ]
    }
   ],
   "source": [
    "# do the learning... average over minibatches of size 5 for maxiter iterations\n",
    "# learn!(obj, learner, infinite_batches(train, size=5))\n",
    "learn!(obj, learner, infinite_obs(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
